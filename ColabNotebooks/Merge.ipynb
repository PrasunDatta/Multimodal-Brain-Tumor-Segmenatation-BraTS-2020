{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Merge.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyNGcxkbbIYOhuKysVnzOjCS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive',force_remount = True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o4qgRLkECmGK","executionInfo":{"status":"ok","timestamp":1651231220531,"user_tz":-360,"elapsed":75888,"user":{"displayName":"Prasun Datta","userId":"15897353971936605969"}},"outputId":"de0e2507-e7de-4ab3-9cba-1bf2fd0c9c81"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["\"\"\"\n","@author: Chenggang\n","@github: https://github.com/MissShihongHowRU\n","@time: 2020-09-09 22:04\n","\"\"\"\n","import torch\n","import torch.nn as nn\n","from siren_pytorch import Sine\n","# from config import config\n","\n","class DownSampling(nn.Module):\n","    # 3x3x3 convolution and 1 padding as default\n","    def __init__(self, inChans, outChans, stride=2, kernel_size=3, padding=1, dropout_rate=None):\n","        super(DownSampling, self).__init__()\n","        \n","        self.dropout_flag = False\n","        self.conv1 = nn.Conv3d(in_channels=inChans, \n","                     out_channels=outChans, \n","                     kernel_size=kernel_size, \n","                     stride=stride,\n","                     padding=padding,\n","                     bias=False)\n","        if dropout_rate is not None:\n","            self.dropout_flag = True\n","            self.dropout = nn.Dropout3d(dropout_rate,inplace=True)\n","            \n","    def forward(self, x):\n","        out = self.conv1(x)\n","        if self.dropout_flag:\n","            out = self.dropout(out)\n","        return out\n","    \n","class EncoderBlock(nn.Module):\n","    '''\n","    Encoder block; Green\n","    '''\n","    def __init__(self, inChans, outChans, stride=1, padding=1, num_groups=8, activation=\"relu\", normalizaiton=\"group_normalization\"):\n","        super(EncoderBlock, self).__init__()\n","        \n","        if normalizaiton == \"group_normalization\":\n","            self.norm1 = nn.GroupNorm(num_groups=num_groups, num_channels=inChans)\n","            self.norm2 = nn.GroupNorm(num_groups=num_groups, num_channels=inChans)\n","        if activation == \"relu\":\n","            self.actv1 = nn.ReLU(inplace=True)\n","            self.actv2 = nn.ReLU(inplace=True)\n","        # elif activation == \"elu\":\n","        #     self.actv1 = nn.ELU(inplace=True)\n","        #     self.actv2 = nn.ELU(inplace=True)\n","        elif activation == \"sin\":\n","            self.actv1 = Sine(1.0)\n","            self.actv2 = Sine(1.0)\n","        self.conv1 = nn.Conv3d(in_channels=inChans, out_channels=outChans, kernel_size=3, stride=stride, padding=padding)\n","        self.conv2 = nn.Conv3d(in_channels=inChans, out_channels=outChans, kernel_size=3, stride=stride, padding=padding)\n","        \n","        \n","    def forward(self, x):\n","        residual = x\n","        \n","        out = self.norm1(x)\n","        out = self.actv1(out)\n","        out = self.conv1(out)\n","        out = self.norm2(out)\n","        out = self.actv2(out)\n","        out = self.conv2(out)\n","        \n","        out += residual\n","        \n","        return out\n","    \n","class LinearUpSampling(nn.Module):\n","    '''\n","    Trilinear interpolate to upsampling\n","    '''\n","    def __init__(self, inChans, outChans, scale_factor=2, mode=\"trilinear\", align_corners=True):\n","        super(LinearUpSampling, self).__init__()\n","        self.scale_factor = scale_factor\n","        self.mode = mode\n","        self.align_corners = align_corners\n","        self.conv1 = nn.Conv3d(in_channels=inChans, out_channels=outChans, kernel_size=1)\n","        # self.conv2 = nn.Conv3d(in_channels=inChans, out_channels=outChans, kernel_size=1)\n","    \n","    def forward(self, x, skipx=None):\n","        out = self.conv1(x)\n","        # out = self.up1(out)\n","        out = nn.functional.interpolate(out, scale_factor=self.scale_factor, mode=self.mode, align_corners=self.align_corners)\n","\n","        if skipx is not None:\n","            out += skipx\n","            # out = torch.cat((out, skipx), 1)\n","            # out = self.conv2(out)  # Given groups=1, weight of size [128, 256, 1, 1, 1], expected input[1, 128, 32, 48, 40] to have 256 channels, but got 128 channels instead\n","\n","\n","        return out\n","    \n","class DecoderBlock(nn.Module):\n","    '''\n","    Decoder block\n","    '''\n","    def __init__(self, inChans, outChans, stride=1, padding=1, num_groups=8, activation=\"relu\", normalizaiton=\"group_normalization\"):\n","        super(DecoderBlock, self).__init__()\n","        \n","        if normalizaiton == \"group_normalization\":\n","            self.norm1 = nn.GroupNorm(num_groups=num_groups, num_channels=outChans)\n","            self.norm2 = nn.GroupNorm(num_groups=num_groups, num_channels=outChans)\n","        if activation == \"relu\":\n","            self.actv1 = nn.ReLU(inplace=True)\n","            self.actv2 = nn.ReLU(inplace=True)\n","        # elif activation == \"elu\":\n","        #     self.actv1 = nn.ELU(inplace=True)\n","        #     self.actv2 = nn.ELU(inplace=True)\n","        elif activation == \"sin\":\n","            self.actv1 = Sine(1.0)\n","            self.actv2 = Sine(1.0)\n","        self.conv1 = nn.Conv3d(in_channels=inChans, out_channels=outChans, kernel_size=3, stride=stride, padding=padding)\n","        self.conv2 = nn.Conv3d(in_channels=outChans, out_channels=outChans, kernel_size=3, stride=stride, padding=padding)\n","        \n","        \n","    def forward(self, x):\n","        residual = x\n","        \n","        out = self.norm1(x)\n","        out = self.actv1(out)\n","        out = self.conv1(out)\n","        out = self.norm2(out)\n","        out = self.actv2(out)\n","        out = self.conv2(out)\n","        \n","        out += residual\n","        \n","        return out\n","    \n","class OutputTransition(nn.Module):\n","    '''\n","    Decoder output layer \n","    output the prediction of segmentation result\n","    '''\n","    def __init__(self, inChans, outChans):\n","        super(OutputTransition, self).__init__()\n","        \n","        self.conv1 = nn.Conv3d(in_channels=inChans, out_channels=outChans, kernel_size=1)\n","        self.actv1 = torch.sigmoid\n","        \n","    def forward(self, x):\n","        return self.actv1(self.conv1(x))\n","\n","class VDResampling(nn.Module):\n","    '''\n","    Variational Auto-Encoder Resampling block\n","    '''\n","    def __init__(self, inChans=256, outChans=256, dense_features=(10,12,8), stride=2, kernel_size=3, padding=1,\n","                 activation=\"relu\", normalizaiton=\"group_normalization\"):\n","        super(VDResampling, self).__init__()\n","        \n","        midChans = int(inChans / 2)\n","        self.dense_features = dense_features\n","        if normalizaiton == \"group_normalization\":\n","            self.gn1 = nn.GroupNorm(num_groups=8, num_channels=inChans)\n","        if activation == \"relu\":\n","            self.actv1 = nn.ReLU(inplace=True)\n","            self.actv2 = nn.ReLU(inplace=True)\n","        # elif activation == \"elu\":\n","        #     self.actv1 = nn.ELU(inplace=True)\n","        #     self.actv2 = nn.ELU(inplace=True)\n","        elif activation == \"sin\":\n","            self.actv1 = Sine(1.0)\n","            self.actv2 = Sine(1.0)\n","        self.conv1 = nn.Conv3d(in_channels=inChans, out_channels=16, kernel_size=kernel_size, stride=stride, padding=padding)\n","        self.dense1 = nn.Linear(in_features=16*dense_features[0]*dense_features[1]*dense_features[2], out_features=256)\n","        self.dense2 = nn.Linear(in_features=128, out_features=128*dense_features[0]*dense_features[1]*dense_features[2])\n","        self.up0 = LinearUpSampling(128, outChans)\n","        \n","    def forward(self, x):\n","        out = self.gn1(x)\n","        out = self.actv1(out)\n","        out = self.conv1(out)   # 16*10*12*8  # 16, 8, 12, 12\n","        print(f\"After Out shape: {out.shape}\")\n","        out = out.view(-1, self.num_flat_features(out))  # flatten  16*8*12*12\n","        print(f\"After flatten : {out.shape}\")\n","        out_vd = self.dense1(out)\n","        print(f\"After dense: {out_vd.shape}\")\n","        distr = out_vd \n","        out = VDraw(out_vd)  # 128\n","        out = self.dense2(out)\n","        out = self.actv2(out)\n","        out = out.view((-1, 128, self.dense_features[0], self.dense_features[1], self.dense_features[2]))  # flat to conv\n","        # out = out.view((1, 128, self.dense_features[0], self.dense_features[1], self.dense_features[2]))\n","        out = self.up0(out)  # include conv1 and upsize 256*20*24*16\n","        \n","        return out, distr\n","        \n","    def num_flat_features(self, x):\n","        size = x.size()[1:]\n","        num_features = 1\n","        for s in size:\n","            num_features *= s\n","            \n","        return num_features\n","\n","def VDraw(x):\n","    # Generate a Gaussian distribution with the given mean(128-d) and std(128-d)\n","    return torch.distributions.Normal(x[:, :128], x[:, 128:]).sample()\n","\n","class VDecoderBlock(nn.Module):\n","    '''\n","    Variational Decoder block\n","    '''\n","    def __init__(self, inChans, outChans, activation=\"relu\", normalizaiton=\"group_normalization\", mode=\"trilinear\"):\n","        super(VDecoderBlock, self).__init__()\n","\n","        self.up0 = LinearUpSampling(inChans, outChans, mode=mode)\n","        self.block = DecoderBlock(outChans, outChans, activation=activation, normalizaiton=normalizaiton)\n","    \n","    def forward(self, x):\n","        out = self.up0(x)\n","        out = self.block(out)\n","\n","        return out\n","\n","class VAE(nn.Module):\n","    '''\n","    Variational Auto-Encoder : to group the features extracted by Encoder\n","    '''\n","    def __init__(self, inChans=256, outChans=4, dense_features=(10, 12, 8),\n","                 activation=\"relu\", normalizaiton=\"group_normalization\", mode=\"trilinear\"):\n","        super(VAE, self).__init__()\n","\n","        self.vd_resample = VDResampling(inChans=inChans, outChans=inChans, dense_features=dense_features)\n","        self.vd_block2 = VDecoderBlock(inChans, inChans//2)\n","        self.vd_block1 = VDecoderBlock(inChans//2, inChans//4)\n","        self.vd_block0 = VDecoderBlock(inChans//4, inChans//8)\n","        self.vd_end = nn.Conv3d(inChans//8, outChans, kernel_size=1)\n","        \n","    def forward(self, x):\n","        out, distr = self.vd_resample(x)\n","        out = self.vd_block2(out)\n","        out = self.vd_block1(out)\n","        out = self.vd_block0(out)\n","        out = self.vd_end(out)\n","\n","        return out, distr\n","\n","#Intermediate_sequential\n","class IntermediateSequential(nn.Sequential):\n","    def __init__(self, *args, return_intermediate=True):\n","        super().__init__(*args)\n","        self.return_intermediate = return_intermediate\n","\n","    def forward(self, input):\n","        if not self.return_intermediate:\n","            return super().forward(input)\n","\n","        intermediate_outputs = {}\n","        output = input\n","        for name, module in self.named_children():\n","            output = intermediate_outputs[name] = module(output)\n","\n","        return output, intermediate_outputs\n","# Transformer \n","class SelfAttention(nn.Module):\n","    def __init__(\n","        self, dim, heads=8, qkv_bias=False, qk_scale=None, dropout_rate=0.0\n","    ):\n","        super().__init__()\n","        self.num_heads = heads\n","        head_dim = dim // heads\n","        self.scale = qk_scale or head_dim ** -0.5\n","\n","        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(dropout_rate)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(dropout_rate)\n","\n","    def forward(self, x):\n","        B, N, C = x.shape\n","        qkv = (\n","            self.qkv(x)\n","            .reshape(B, N, 3, self.num_heads, C // self.num_heads)\n","            .permute(2, 0, 3, 1, 4)\n","        )\n","        q, k, v = (\n","            qkv[0],\n","            qkv[1],\n","            qkv[2],\n","        )  # make torchscript happy (cannot use tensor as tuple)\n","\n","        attn = (q @ k.transpose(-2, -1)) * self.scale\n","        attn = attn.softmax(dim=-1)\n","        attn = self.attn_drop(attn)\n","\n","        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        return x\n","\n","\n","class Residual(nn.Module):\n","    def __init__(self, fn):\n","        super().__init__()\n","        self.fn = fn\n","\n","    def forward(self, x):\n","        return self.fn(x) + x\n","\n","\n","class PreNorm(nn.Module):\n","    def __init__(self, dim, fn):\n","        super().__init__()\n","        self.norm = nn.LayerNorm(dim)\n","        self.fn = fn\n","\n","    def forward(self, x):\n","        return self.fn(self.norm(x))\n","\n","\n","class PreNormDrop(nn.Module):\n","    def __init__(self, dim, dropout_rate, fn):\n","        super().__init__()\n","        self.norm = nn.LayerNorm(dim)\n","        self.dropout = nn.Dropout(p=dropout_rate)\n","        self.fn = fn\n","\n","    def forward(self, x):\n","        return self.dropout(self.fn(self.norm(x)))\n","\n","from torch import Tensor\n","import torch.nn.functional as F\n","\n","class GELU(nn.Module):\n","    def forward(self, input: Tensor) -> Tensor:\n","        return F.gelu(input)\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, dim, hidden_dim, dropout_rate):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(dim, hidden_dim),\n","            GELU(),\n","            nn.Dropout(p=dropout_rate),\n","            nn.Linear(hidden_dim, dim),\n","            nn.Dropout(p=dropout_rate),\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","\n","class TransformerModel(nn.Module):\n","    def __init__(\n","        self,\n","        dim,\n","        depth,\n","        heads,\n","        mlp_dim,\n","        dropout_rate=0.1,\n","        attn_dropout_rate=0.1,\n","    ):\n","        super().__init__()\n","        layers = []\n","        for _ in range(depth):\n","            layers.extend(\n","                [\n","                    Residual(\n","                        PreNormDrop(\n","                            dim,\n","                            dropout_rate,\n","                            SelfAttention(dim, heads=heads, dropout_rate=attn_dropout_rate),\n","                        )\n","                    ),\n","                    Residual(\n","                        PreNorm(dim, FeedForward(dim, mlp_dim, dropout_rate))\n","                    ),\n","                ]\n","            )\n","            # dim = dim / 2\n","        self.net = IntermediateSequential(*layers)\n","\n","\n","    def forward(self, x):\n","        return self.net(x)\n","# Transformer Segment Ends Here.\n","#Position Encoding \n","class FixedPositionalEncoding(nn.Module):\n","    def __init__(self, embedding_dim, max_length=512):\n","        super(FixedPositionalEncoding, self).__init__()\n","\n","        pe = torch.zeros(max_length, embedding_dim)\n","        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(\n","            torch.arange(0, embedding_dim, 2).float()\n","            * (-torch.log(torch.tensor(10000.0)) / embedding_dim)\n","        )\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[: x.size(0), :]\n","        return x\n","\n","\n","class LearnedPositionalEncoding(nn.Module):\n","    def __init__(self, max_position_embeddings, embedding_dim, seq_length):\n","        super(LearnedPositionalEncoding, self).__init__()\n","\n","        self.position_embeddings = nn.Parameter(torch.zeros(1, 7680, 512)) #8x\n","\n","    def forward(self, x, position_ids=None):\n","\n","        position_embeddings = self.position_embeddings\n","        return x + position_embeddings\n","# Position encoding ends here.\n","\n","#After transformer 2 enblocks \n","class EnBlock1(nn.Module):\n","    def __init__(self, in_channels):\n","        super(EnBlock1, self).__init__()\n","\n","        self.bn1 = nn.BatchNorm3d(512 // 2)\n","        self.relu1 = nn.ReLU(inplace=True)\n","        self.bn2 = nn.BatchNorm3d(512 // 2)\n","        self.relu2 = nn.ReLU(inplace=True)\n","        self.conv1 = nn.Conv3d(in_channels, in_channels // 2, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv3d(in_channels // 2, in_channels // 2, kernel_size=3, padding=1)\n","\n","    def forward(self, x):\n","        x1 = self.conv1(x)\n","        x1 = self.bn1(x1)\n","        x1 = self.relu1(x1)\n","        x1 = self.conv2(x1)\n","        x1 = self.bn2(x1)\n","        x1 = self.relu2(x1)\n","\n","        return x1\n","\n","\n","class EnBlock2(nn.Module):\n","    def __init__(self, in_channels):\n","        super(EnBlock2, self).__init__()\n","\n","        self.conv1 = nn.Conv3d(in_channels, in_channels, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm3d(512 // 2)\n","        self.relu1 = nn.ReLU(inplace=True)\n","        self.bn2 = nn.BatchNorm3d(512 // 2)\n","        self.relu2 = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv3d(in_channels, in_channels, kernel_size=3, padding=1)\n","\n","    def forward(self, x):\n","        x1 = self.conv1(x)\n","        x1 = self.bn1(x1)\n","        x1 = self.relu1(x1)\n","        x1 = self.conv2(x1)\n","        x1 = self.bn2(x1)\n","        x1 = self.relu2(x1)\n","        x1 = x1 + x\n","\n","        return x1\n","\n","\n","class NvNet(nn.Module):\n","    def __init__(self, config):\n","        super(NvNet, self).__init__()\n","        \n","        self.config = config\n","        # some critical parameters\n","        self.inChans = config[\"input_shape\"][1]\n","        self.input_shape = config[\"input_shape\"]\n","        self.seg_outChans = 3\n","        # self.seg_outChans = config[\"n_labels\"]\n","        self.activation = config[\"activation\"]\n","        self.normalizaiton = config[\"normalizaiton\"]\n","        self.mode = config[\"mode\"]\n","\n","        #Transformer Layer Addition\n","        #super(TransformerBTS, self).__init__()\n","\n","        # assert embedding_dim % num_heads == 0\n","        # assert img_dim % patch_dim == 0\n","\n","        self.img_dim = config[\"img_dim\"]\n","        self.embedding_dim = config[\"embedding_dim\"]\n","        self.num_heads = config[\"num_heads\"]\n","        self.patch_dim = config[\"patch_dim\"]\n","        self.num_channels = config[\"num_channels\"]\n","        self.dropout_rate = config[\"dropout_rate\"]\n","        self.attn_dropout_rate = config[\"attn_dropout_rate\"]\n","        self.num_layers = config[\"num_layers\"]\n","        self.hidden_dim = config[\"hidden_dim\"]\n","        # self.conv_patch_representation = conv_patch_representation\n","\n","        self.num_patches = int((self.img_dim // self.patch_dim) ** 3)\n","        self.seq_length = self.num_patches\n","        # self.flatten_dim = 128 * num_channels\n","        self.positional_encoding_type = config[\"positional_encoding_type\"] \n","        # self.linear_encoding = nn.Linear(self.flatten_dim, self.embedding_dim)\n","        if self.positional_encoding_type == \"learned\":\n","            self.position_encoding = LearnedPositionalEncoding(\n","                self.seq_length, self.embedding_dim, self.seq_length\n","            )\n","        elif self.positional_encoding_type == \"fixed\":\n","            self.position_encoding = FixedPositionalEncoding(\n","                self.embedding_dim,\n","            )\n","\n","        self.pe_dropout = nn.Dropout(p=self.dropout_rate)\n","\n","        self.transformer = TransformerModel(\n","            self.embedding_dim,\n","            self.num_layers,\n","            self.num_heads,\n","            self.hidden_dim,\n","\n","            self.dropout_rate,\n","            self.attn_dropout_rate,\n","        )\n","        self.pre_head_ln = nn.LayerNorm(self.embedding_dim)\n","\n","        # if self.conv_patch_representation:\n","\n","        self.conv_x = nn.Conv3d(\n","                256,\n","                self.embedding_dim,\n","                kernel_size=3,\n","                stride=1,\n","                padding=1\n","            )\n","\n","        # self.Unet = Unet(in_channels=4, base_channels=16, num_classes=4)\n","        # self.gn1 = nn.GroupNorm(num_groups=8, num_channels= 256)\n","        self.bn = nn.BatchNorm3d(256)\n","        self.relu = nn.ReLU(inplace=True)\n","        \n","        # Encoder Blocks\n","        self.in_conv0 = DownSampling(inChans=self.inChans, outChans=32, stride=1, dropout_rate=0.2)\n","        self.en_block0 = EncoderBlock(32, 32, activation=self.activation, normalizaiton=self.normalizaiton)\n","        self.en_down1 = DownSampling(32, 64)\n","        self.en_block1_0 = EncoderBlock(64, 64, activation=self.activation, normalizaiton=self.normalizaiton)\n","        self.en_block1_1 = EncoderBlock(64, 64, activation=self.activation, normalizaiton=self.normalizaiton)\n","        self.en_down2 = DownSampling(64, 128)\n","        self.en_block2_0 = EncoderBlock(128, 128, activation=self.activation, normalizaiton=self.normalizaiton)\n","        self.en_block2_1 = EncoderBlock(128, 128, activation=self.activation, normalizaiton=self.normalizaiton)\n","        self.en_down3 = DownSampling(128, 256)\n","        self.en_block3_0 = EncoderBlock(256, 256, activation=self.activation, normalizaiton=self.normalizaiton)\n","        self.en_block3_1 = EncoderBlock(256, 256, activation=self.activation, normalizaiton=self.normalizaiton)\n","        self.en_block3_2 = EncoderBlock(256, 256, activation=self.activation, normalizaiton=self.normalizaiton)\n","        self.en_block3_3 = EncoderBlock(256, 256, activation=self.activation, normalizaiton=self.normalizaiton)\n","        \n","        # Decoder Blocks\n","        self.de_up2 = LinearUpSampling(256, 128, mode=self.mode)\n","        self.de_block2 = DecoderBlock(128, 128, activation=self.activation, normalizaiton=self.normalizaiton)\n","        self.de_up1 = LinearUpSampling(128, 64, mode=self.mode)\n","        self.de_block1 = DecoderBlock(64, 64, activation=self.activation, normalizaiton=self.normalizaiton)\n","        self.de_up0 = LinearUpSampling(64, 32, mode=self.mode)\n","        self.de_block0 = DecoderBlock(32, 32, activation=self.activation, normalizaiton=self.normalizaiton)\n","        self.de_end = OutputTransition(32, self.seg_outChans)\n","        \n","        # Variational Auto-Encoder\n","        if self.config[\"VAE_enable\"]:\n","            self.dense_features = (self.input_shape[2]//16, self.input_shape[3]//16, self.input_shape[4]//16)  # 8, 12, 12\n","            self.vae = VAE(256, outChans=self.inChans, dense_features=self.dense_features)\n","        #After Transformer Layer \n","        self.Enblock8_1 = EnBlock1(in_channels=self.embedding_dim)\n","        self.Enblock8_2 = EnBlock2(in_channels=self.embedding_dim // 2)    \n","    def decode(self,x_tr,intmd_x, intmd_layers=[1, 2, 3, 4]):\n","\n","        assert intmd_layers is not None, \"pass the intermediate layers for MLA\"\n","        encoder_outputs = {}\n","        all_keys = []\n","        for i in intmd_layers:\n","            val = str(2 * i - 1)\n","            _key = 'Z' + str(i)\n","            all_keys.append(_key)\n","            encoder_outputs[_key] = intmd_x[val]\n","        all_keys.reverse()\n","        return encoder_outputs[all_keys[0]]\n","\n","    def _reshape_output(self, x):\n","        x = x.view(\n","            x.size(0),\n","            int(self.input_shape[2] / self.patch_dim),\n","            int(self.input_shape[3] / self.patch_dim),\n","            int(self.input_shape[4] / self.patch_dim),\n","            self.embedding_dim,\n","        )\n","        x = x.permute(0, 4, 1, 2, 3).contiguous()\n","\n","        return x\n","\n","    def forward(self, x):\n","        out_init = self.in_conv0(x)  # 32, 128, 192, 192\n","        out_en0 = self.en_block0(out_init)  # 32, 128, 192, 192\n","        out_en1 = self.en_block1_1(self.en_block1_0(self.en_down1(out_en0)))  # 64, 64, 96, 96\n","        out_en2 = self.en_block2_1(self.en_block2_0(self.en_down2(out_en1)))  # 128, 32, 48, 48\n","        out_en3 = self.en_block3_3(self.en_block3_2(self.en_block3_1(self.en_block3_0(self.en_down3(out_en2)))))  # 256, 16, 24, 24\n","        transformer_input = out_en3\n","        print(f\"Transformer_input: {transformer_input.shape}\")\n","        x_tr = self.bn(transformer_input) #x = transformer_input\n","        x_tr = self.relu(x_tr)\n","        x_tr = self.conv_x(x_tr)\n","        x_tr = x_tr.permute(0, 2, 3, 4, 1).contiguous()\n","        x_tr = x_tr.view(x_tr.size(0), -1, self.embedding_dim)\n","        print(f\"Before position embedding: {x_tr.shape}\")\n","        x_tr = self.position_encoding(x_tr)\n","        x_tr = self.pe_dropout(x_tr)\n","\n","        # apply transformer\n","        x_tr, intmd_x = self.transformer(x_tr)\n","        x_tr = self.pre_head_ln(x_tr)\n","\n"," \n","\n","        x8 = self.decode(x_tr,intmd_x,intmd_layers=[1, 2, 3, 4])\n","\n","        x8 = self._reshape_output(x8)\n","        print(f\"End_Trans_After_rehape: {x8.shape}\")\n","        x8 = self.Enblock8_1(x8)\n","        x8 = self.Enblock8_2(x8)\n","        print(f\"After Enblocks_Transformer Output: {x8.shape}\")\n","        out_de2 = self.de_block2(self.de_up2(x8, out_en2))   # forward\n","        out_de1 = self.de_block1(self.de_up1(out_de2, out_en1))\n","        out_de0 = self.de_block0(self.de_up0(out_de1, out_en0))\n","        out_end = self.de_end(out_de0)\n","\n","\n","        \n","        if self.config[\"VAE_enable\"]:\n","            out_vae, out_distr = self.vae(x8)\n","            out_final = torch.cat((out_end, out_vae), 1)\n","            return out_final, out_distr\n","        \n","        return out_end\n","\n","# if __name__ == '__main__':\n","#     with torch.no_grad():\n","#         import os\n","#         os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n","#         cuda0 = torch.device('cuda:0')\n","#         x = torch.rand((1, 4, 128, 192, 160), device=cuda0)\n","#         # _, model = TransBTS(dataset='brats', _conv_repr=True, _pe_type=\"learned\")\n","#         model = NvNet(config=config)\n","#         model.cuda()\n","#         y, distr = model(x)\n","#         print(y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":956},"id":"7CJJHmm5Hqff","executionInfo":{"status":"ok","timestamp":1651224751422,"user_tz":-360,"elapsed":4423,"user":{"displayName":"Prasun Datta","userId":"15897353971936605969"}},"outputId":"084be4bc-07ba-4738-8f80-71d2e9b3f25c"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: batchgenerators==0.20.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 1)) (0.20.1)\n","Requirement already satisfied: certifi==2016.2.28 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 2)) (2016.2.28)\n","Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 3)) (0.10.0)\n","Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 4)) (4.4.2)\n","Requirement already satisfied: et-xmlfile==1.0.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 5)) (1.0.1)\n","Requirement already satisfied: future==0.18.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 6)) (0.18.2)\n","Requirement already satisfied: imageio==2.9.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 7)) (2.9.0)\n","Requirement already satisfied: jdcal==1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 8)) (1.4.1)\n","Requirement already satisfied: joblib==0.15.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 9)) (0.15.1)\n","Requirement already satisfied: kiwisolver==1.2.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 10)) (1.2.0)\n","Requirement already satisfied: linecache2==1.0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 11)) (1.0.0)\n","Requirement already satisfied: matplotlib==3.2.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 12)) (3.2.1)\n","Requirement already satisfied: networkx==2.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 13)) (2.5)\n","Requirement already satisfied: nibabel==3.1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 14)) (3.1.0)\n","Requirement already satisfied: nilearn==0.6.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 15)) (0.6.2)\n","Requirement already satisfied: numpy==1.18.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 16)) (1.18.4)\n","Requirement already satisfied: opencv-python==4.2.0.34 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 17)) (4.2.0.34)\n","Requirement already satisfied: openpyxl==3.0.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 18)) (3.0.4)\n","Requirement already satisfied: packaging==20.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 19)) (20.4)\n","Requirement already satisfied: pandas==1.0.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 20)) (1.0.5)\n","Requirement already satisfied: Pillow==7.0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 21)) (7.0.0)\n","Requirement already satisfied: protobuf==3.12.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 22)) (3.12.2)\n","Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 23)) (2.4.7)\n","Requirement already satisfied: python-dateutil==2.8.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 24)) (2.8.1)\n","Requirement already satisfied: pytz==2020.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 25)) (2020.1)\n","Requirement already satisfied: PyWavelets==1.1.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 26)) (1.1.1)\n","Requirement already satisfied: scikit-image==0.17.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 27)) (0.17.2)\n","Requirement already satisfied: scikit-learn==0.23.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 28)) (0.23.1)\n","Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 29)) (1.4.1)\n","Requirement already satisfied: seaborn==0.11.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 30)) (0.11.0)\n","Requirement already satisfied: SimpleITK==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 31)) (1.1.0)\n","Requirement already satisfied: siren-pytorch==0.0.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 32)) (0.0.3)\n","Requirement already satisfied: six==1.15.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 33)) (1.15.0)\n","Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 34)) (0.0)\n","Requirement already satisfied: tensorboardX==1.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 35)) (1.4)\n","Requirement already satisfied: threadpoolctl==2.0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 36)) (2.0.0)\n","Requirement already satisfied: tifffile==2020.9.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 37)) (2020.9.3)\n","Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 38)) (1.6.0)\n","Requirement already satisfied: torchvision==0.7.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 39)) (0.7.0)\n","Requirement already satisfied: tqdm==4.26.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 40)) (4.26.0)\n","Requirement already satisfied: traceback2==1.4.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 41)) (1.4.0)\n","Requirement already satisfied: unittest2==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 42)) (1.1.0)\n","Requirement already satisfied: xlrd==1.2.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 43)) (1.2.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf==3.12.2->-r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 22)) (57.4.0)\n","Collecting argparse\n","  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n","Installing collected packages: argparse\n","Successfully installed argparse-1.4.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["argparse"]}}},"metadata":{}}]},{"cell_type":"code","source":["\"\"\"\n","@author: Chenggang\n","@github: https://github.com/MissShihongHowRU\n","@time: 2020-09-09 22:04\n","\"\"\"\n","import torch\n","import torch.nn as nn\n","from siren_pytorch import Sine\n","import torch.nn.functional as F\n","\n","class DownSampling(nn.Module):\n","    # 3x3x3 convolution and 1 padding as default\n","    def __init__(self, inChans, outChans, stride=2, kernel_size=3, padding=1, dropout_rate=None):\n","        super(DownSampling, self).__init__()\n","        \n","        self.dropout_flag = False\n","        self.conv1 = nn.Conv3d(in_channels=inChans, \n","                     out_channels=outChans, \n","                     kernel_size=kernel_size, \n","                     stride=stride,\n","                     padding=padding,\n","                     bias=False)\n","        if dropout_rate is not None:\n","            self.dropout_flag = True\n","            self.dropout = nn.Dropout3d(dropout_rate,inplace=True)\n","            \n","    def forward(self, x):\n","        out = self.conv1(x)\n","        if self.dropout_flag:\n","            out = self.dropout(out)\n","        return out\n","    \n","class EncoderBlock(nn.Module):\n","    '''\n","    Encoder block; Green\n","    '''\n","    def __init__(self, inChans, outChans, stride=1, padding=1, num_groups=8, activation=\"relu\", normalizaiton=\"group_normalization\"):\n","        super(EncoderBlock, self).__init__()\n","        \n","        if normalizaiton == \"group_normalization\":\n","            self.norm1 = nn.GroupNorm(num_groups=num_groups, num_channels=inChans)\n","            self.norm2 = nn.GroupNorm(num_groups=num_groups, num_channels=inChans)\n","        if activation == \"relu\":\n","            self.actv1 = nn.ReLU(inplace=True)\n","            self.actv2 = nn.ReLU(inplace=True)\n","        # elif activation == \"elu\":\n","        #     self.actv1 = nn.ELU(inplace=True)\n","        #     self.actv2 = nn.ELU(inplace=True)\n","        elif activation == \"sin\":\n","            self.actv1 = Sine(1.0)\n","            self.actv2 = Sine(1.0)\n","        self.conv1 = nn.Conv3d(in_channels=inChans, out_channels=outChans, kernel_size=3, stride=stride, padding=padding)\n","        self.conv2 = nn.Conv3d(in_channels=inChans, out_channels=outChans, kernel_size=3, stride=stride, padding=padding)\n","        \n","        \n","    def forward(self, x):\n","        residual = x\n","        \n","        out = self.norm1(x)\n","        out = self.actv1(out)\n","        out = self.conv1(out)\n","        out = self.norm2(out)\n","        out = self.actv2(out)\n","        out = self.conv2(out)\n","        \n","        out += residual\n","        \n","        return out\n","\n","\n","class AttentionBlock(nn.Module):\n","    '''\n","    stride = 2; please set --attention parameter to 1 when activate this block. the result name should include \"att2\".\n","    To fit in the structure of the V-net: F_l = F_int\n","    '''\n","    def __init__(self, F_g, F_l, F_int, scale_factor=2, mode=\"trilinear\"):\n","        super(AttentionBlock, self).__init__()\n","        self.scale_factor = scale_factor\n","        self.mode = mode\n","        self.W_g = nn.Sequential(\n","            nn.Conv3d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),  # reduce num_channels\n","            nn.BatchNorm3d(F_int)\n","        )\n","\n","        self.W_x = nn.Sequential(\n","            nn.Conv3d(F_l, F_int, kernel_size=scale_factor, stride=scale_factor, padding=0, bias=True),  # downsize\n","            nn.BatchNorm3d(F_int)\n","        )\n","\n","        self.psi = nn.Sequential(\n","            nn.Conv3d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n","            nn.BatchNorm3d(1),\n","            nn.Sigmoid()\n","        )\n","\n","        self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, g, x, visualize=False):\n","        '''\n","\n","        :param g: gate signal from coarser scale\n","        :param x: the output of the l-th layer in the encoder\n","        :param visualize: enable this when plotting attention matrix\n","        :return:\n","        '''\n","        x1 = self.W_x(x)\n","        g1 = self.W_g(g)\n","        relu = self.relu(g1 + x1)\n","        sig = self.psi(relu)\n","        alpha = nn.functional.interpolate(sig, scale_factor=self.scale_factor, mode=self.mode)\n","\n","        if visualize:\n","            return alpha\n","        else:\n","            return x * alpha\n","\n","\n","class LinearUpSampling(nn.Module):\n","    '''\n","    Trilinear interpolate to upsampling\n","    '''\n","    def __init__(self, inChans, outChans, scale_factor=2, mode=\"trilinear\", align_corners=True):\n","        super(LinearUpSampling, self).__init__()\n","        self.scale_factor = scale_factor\n","        self.mode = mode\n","        self.align_corners = align_corners\n","        self.conv1 = nn.Conv3d(in_channels=inChans, out_channels=outChans, kernel_size=1)\n","        # self.conv2 = nn.Conv3d(in_channels=inChans, out_channels=outChans, kernel_size=1)\n","    \n","    def forward(self, x, skipx=None):\n","        out = self.conv1(x)  # reduce num_channels\n","        out = nn.functional.interpolate(out, scale_factor=self.scale_factor, mode=self.mode, align_corners=self.align_corners)\n","        # up-size * 2\n","\n","        if skipx is not None:\n","            out += skipx\n","\n","        return out\n","    \n","class DecoderBlock(nn.Module):\n","    '''\n","    Decoder block\n","    '''\n","    def __init__(self, inChans, outChans, stride=1, padding=1, num_groups=8, activation=\"relu\", normalizaiton=\"group_normalization\"):\n","        super(DecoderBlock, self).__init__()\n","        \n","        if normalizaiton == \"group_normalization\":\n","            self.norm1 = nn.GroupNorm(num_groups=num_groups, num_channels=outChans)\n","            self.norm2 = nn.GroupNorm(num_groups=num_groups, num_channels=outChans)\n","        if activation == \"relu\":\n","            self.actv1 = nn.ReLU(inplace=True)\n","            self.actv2 = nn.ReLU(inplace=True)\n","        # elif activation == \"elu\":\n","        #     self.actv1 = nn.ELU(inplace=True)\n","        #     self.actv2 = nn.ELU(inplace=True)\n","        elif activation == \"sin\":\n","            self.actv1 = Sine(1.0)\n","            self.actv2 = Sine(1.0)\n","        self.conv1 = nn.Conv3d(in_channels=inChans, out_channels=outChans, kernel_size=3, stride=stride, padding=padding)\n","        self.conv2 = nn.Conv3d(in_channels=outChans, out_channels=outChans, kernel_size=3, stride=stride, padding=padding)\n","        \n","        \n","    def forward(self, x):\n","        residual = x\n","        \n","        out = self.norm1(x)\n","        out = self.actv1(out)\n","        out = self.conv1(out)\n","        out = self.norm2(out)\n","        out = self.actv2(out)\n","        out = self.conv2(out)\n","        \n","        out += residual\n","        \n","        return out\n","    \n","class OutputTransition(nn.Module):\n","    '''\n","    Decoder output layer \n","    output the prediction of segmentation result\n","    '''\n","    def __init__(self, inChans, outChans):\n","        super(OutputTransition, self).__init__()\n","        \n","        self.conv1 = nn.Conv3d(in_channels=inChans, out_channels=outChans, kernel_size=1)\n","        self.actv1 = torch.sigmoid\n","        \n","    def forward(self, x):\n","        return self.actv1(self.conv1(x))\n","\n","class VDResampling(nn.Module):\n","    '''\n","    Variational Auto-Encoder Resampling block\n","    '''\n","    def __init__(self, inChans=256, outChans=256, dense_features=(10, 12, 8), stride=2, kernel_size=3, padding=1,\n","                 activation=\"relu\", normalizaiton=\"group_normalization\"):\n","        super(VDResampling, self).__init__()\n","        \n","        midChans = int(inChans / 2)\n","        self.dense_features = dense_features\n","        if normalizaiton == \"group_normalization\":\n","            self.gn1 = nn.GroupNorm(num_groups=8, num_channels=inChans)\n","        if activation == \"relu\":\n","            self.actv1 = nn.ReLU(inplace=True)\n","            self.actv2 = nn.ReLU(inplace=True)\n","        elif activation == \"sin\":\n","            self.actv1 = Sine(1.0)\n","            self.actv2 = Sine(1.0)\n","        self.conv1 = nn.Conv3d(in_channels=inChans, out_channels=16, kernel_size=kernel_size, stride=stride, padding=padding)\n","        self.dense1 = nn.Linear(in_features=16*dense_features[0]*dense_features[1]*dense_features[2], out_features=256)\n","        self.dense2 = nn.Linear(in_features=128, out_features=128*dense_features[0]*dense_features[1]*dense_features[2])\n","        self.up0 = LinearUpSampling(128, outChans)\n","        \n","    def forward(self, x):\n","        out = self.gn1(x)\n","        out = self.actv1(out)\n","        out = self.conv1(out)   # 16*10*12*8\n","        out = out.view(-1, self.num_flat_features(out))  # flatten\n","        out_vd = self.dense1(out)\n","        distr = out_vd \n","        out = VDraw(out_vd)  # 128\n","        out = self.dense2(out)\n","        out = self.actv2(out)\n","        out = out.view((-1, 128, self.dense_features[0], self.dense_features[1], self.dense_features[2]))  # flat to conv\n","        # out = out.view((1, 128, self.dense_features[0], self.dense_features[1], self.dense_features[2]))\n","        out = self.up0(out)  # include conv1 and upsize 256*20*24*16\n","        \n","        return out, distr\n","        \n","    def num_flat_features(self, x):\n","        size = x.size()[1:]\n","        num_features = 1\n","        for s in size:\n","            num_features *= s\n","            \n","        return num_features\n","\n","def VDraw(x):\n","    # Generate a Gaussian distribution with the given mean(128-d) and std(128-d)\n","    return torch.distributions.Normal(x[:, :128], x[:, 128:]).sample()\n","\n","class VDecoderBlock(nn.Module):\n","    '''\n","    Variational Decoder block\n","    '''\n","    def __init__(self, inChans, outChans, activation=\"relu\", normalizaiton=\"group_normalization\", mode=\"trilinear\"):\n","        super(VDecoderBlock, self).__init__()\n","\n","        self.up0 = LinearUpSampling(inChans, outChans, mode=mode)\n","        self.block = DecoderBlock(outChans, outChans, activation=activation, normalizaiton=normalizaiton)\n","    \n","    def forward(self, x):\n","        out = self.up0(x)\n","        out = self.block(out)\n","\n","        return out\n","\n","class VAE(nn.Module):\n","    '''\n","    Variational Auto-Encoder : to group the features extracted by Encoder\n","    '''\n","    def __init__(self, inChans=256, outChans=4, dense_features=(10, 12, 8),\n","                 activation=\"relu\", normalizaiton=\"group_normalization\", mode=\"trilinear\"):\n","        super(VAE, self).__init__()\n","\n","        self.vd_resample = VDResampling(inChans=inChans, outChans=inChans, dense_features=dense_features)\n","        self.vd_block2 = VDecoderBlock(inChans, inChans//2)\n","        self.vd_block1 = VDecoderBlock(inChans//2, inChans//4)\n","        self.vd_block0 = VDecoderBlock(inChans//4, inChans//8)\n","        self.vd_end = nn.Conv3d(inChans//8, outChans, kernel_size=1)\n","        \n","    def forward(self, x):\n","        out, distr = self.vd_resample(x)\n","        out = self.vd_block2(out)\n","        out = self.vd_block1(out)\n","        out = self.vd_block0(out)\n","        out = self.vd_end(out)\n","\n","        return out, distr\n","\n","#Intermediate_sequential\n","class IntermediateSequential(nn.Sequential):\n","    def __init__(self, *args, return_intermediate=True):\n","        super().__init__(*args)\n","        self.return_intermediate = return_intermediate\n","\n","    def forward(self, input):\n","        if not self.return_intermediate:\n","            return super().forward(input)\n","\n","        intermediate_outputs = {}\n","        output = input\n","        for name, module in self.named_children():\n","            output = intermediate_outputs[name] = module(output)\n","\n","        return output, intermediate_outputs\n","# Transformer \n","class SelfAttention(nn.Module):\n","    def __init__(\n","        self, dim, heads=8, qkv_bias=False, qk_scale=None, dropout_rate=0.0\n","    ):\n","        super().__init__()\n","        self.num_heads = heads\n","        head_dim = dim // heads\n","        self.scale = qk_scale or head_dim ** -0.5\n","\n","        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(dropout_rate)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(dropout_rate)\n","\n","    def forward(self, x):\n","        B, N, C = x.shape\n","        qkv = (\n","            self.qkv(x)\n","            .reshape(B, N, 3, self.num_heads, C // self.num_heads)\n","            .permute(2, 0, 3, 1, 4)\n","        )\n","        q, k, v = (\n","            qkv[0],\n","            qkv[1],\n","            qkv[2],\n","        )  # make torchscript happy (cannot use tensor as tuple)\n","\n","        attn = (q @ k.transpose(-2, -1)) * self.scale\n","        attn = attn.softmax(dim=-1)\n","        attn = self.attn_drop(attn)\n","\n","        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        return x\n","\n","\n","class Residual(nn.Module):\n","    def __init__(self, fn):\n","        super().__init__()\n","        self.fn = fn\n","\n","    def forward(self, x):\n","        return self.fn(x) + x\n","\n","\n","class PreNorm(nn.Module):\n","    def __init__(self, dim, fn):\n","        super().__init__()\n","        self.norm = nn.LayerNorm(dim)\n","        self.fn = fn\n","\n","    def forward(self, x):\n","        return self.fn(self.norm(x))\n","\n","\n","class PreNormDrop(nn.Module):\n","    def __init__(self, dim, dropout_rate, fn):\n","        super().__init__()\n","        self.norm = nn.LayerNorm(dim)\n","        self.dropout = nn.Dropout(p=dropout_rate)\n","        self.fn = fn\n","\n","    def forward(self, x):\n","        return self.dropout(self.fn(self.norm(x)))\n","\n","from torch import Tensor\n","import torch.nn.functional as F\n","\n","class GELU(nn.Module):\n","    def forward(self, input: Tensor) -> Tensor:\n","        return F.gelu(input)\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, dim, hidden_dim, dropout_rate):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(dim, hidden_dim),\n","            GELU(),\n","            nn.Dropout(p=dropout_rate),\n","            nn.Linear(hidden_dim, dim),\n","            nn.Dropout(p=dropout_rate),\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","\n","class TransformerModel(nn.Module):\n","    def __init__(\n","        self,\n","        dim,\n","        depth,\n","        heads,\n","        mlp_dim,\n","        dropout_rate=0.1,\n","        attn_dropout_rate=0.1,\n","    ):\n","        super().__init__()\n","        layers = []\n","        for _ in range(depth):\n","            layers.extend(\n","                [\n","                    Residual(\n","                        PreNormDrop(\n","                            dim,\n","                            dropout_rate,\n","                            SelfAttention(dim, heads=heads, dropout_rate=attn_dropout_rate),\n","                        )\n","                    ),\n","                    Residual(\n","                        PreNorm(dim, FeedForward(dim, mlp_dim, dropout_rate))\n","                    ),\n","                ]\n","            )\n","            # dim = dim / 2\n","        self.net = IntermediateSequential(*layers)\n","\n","\n","    def forward(self, x):\n","        return self.net(x)\n","# Transformer Segment Ends Here.\n","#Position Encoding \n","class FixedPositionalEncoding(nn.Module):\n","    def __init__(self, embedding_dim, max_length=512):\n","        super(FixedPositionalEncoding, self).__init__()\n","\n","        pe = torch.zeros(max_length, embedding_dim)\n","        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(\n","            torch.arange(0, embedding_dim, 2).float()\n","            * (-torch.log(torch.tensor(10000.0)) / embedding_dim)\n","        )\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[: x.size(0), :]\n","        return x\n","\n","\n","class LearnedPositionalEncoding(nn.Module):\n","    def __init__(self, max_position_embeddings, embedding_dim, seq_length):\n","        super(LearnedPositionalEncoding, self).__init__()\n","\n","        self.position_embeddings = nn.Parameter(torch.zeros(1, 7680, 512)) #8x\n","\n","    def forward(self, x, position_ids=None):\n","\n","        position_embeddings = self.position_embeddings\n","        return x + position_embeddings\n","# Position encoding ends here.\n","\n","#After transformer 2 enblocks \n","class EnBlock1(nn.Module):\n","    def __init__(self, in_channels):\n","        super(EnBlock1, self).__init__()\n","\n","        self.bn1 = nn.BatchNorm3d(512 // 2)\n","        self.relu1 = nn.ReLU(inplace=True)\n","        self.bn2 = nn.BatchNorm3d(512 // 2)\n","        self.relu2 = nn.ReLU(inplace=True)\n","        self.conv1 = nn.Conv3d(in_channels, in_channels // 2, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv3d(in_channels // 2, in_channels // 2, kernel_size=3, padding=1)\n","\n","    def forward(self, x):\n","        x1 = self.conv1(x)\n","        x1 = self.bn1(x1)\n","        x1 = self.relu1(x1)\n","        x1 = self.conv2(x1)\n","        x1 = self.bn2(x1)\n","        x1 = self.relu2(x1)\n","\n","        return x1\n","\n","\n","class EnBlock2(nn.Module):\n","    def __init__(self, in_channels):\n","        super(EnBlock2, self).__init__()\n","\n","        self.conv1 = nn.Conv3d(in_channels, in_channels, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm3d(512 // 2)\n","        self.relu1 = nn.ReLU(inplace=True)\n","        self.bn2 = nn.BatchNorm3d(512 // 2)\n","        self.relu2 = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv3d(in_channels, in_channels, kernel_size=3, padding=1)\n","\n","    def forward(self, x):\n","        x1 = self.conv1(x)\n","        x1 = self.bn1(x1)\n","        x1 = self.relu1(x1)\n","        x1 = self.conv2(x1)\n","        x1 = self.bn2(x1)\n","        x1 = self.relu2(x1)\n","        x1 = x1 + x\n","\n","        return x1\n","\n","\n","class AttentionVNet(nn.Module):\n","    def __init__(self, config):\n","        super(AttentionVNet, self).__init__()\n","        \n","        self.config = config\n","        # some critical parameters\n","        self.inChans = config[\"input_shape\"][1]\n","        self.input_shape = config[\"input_shape\"]\n","        self.seg_outChans = 3\n","        self.activation = config[\"activation\"]\n","        self.normalizaiton = config[\"normalizaiton\"]\n","        self.mode = config[\"mode\"]\n","        #Transformer Layer Addition\n","        self.img_dim = config[\"img_dim\"]\n","        self.embedding_dim = config[\"embedding_dim\"]\n","        self.num_heads = config[\"num_heads\"]\n","        self.patch_dim = config[\"patch_dim\"]\n","        self.num_channels = config[\"num_channels\"]\n","        self.dropout_rate = config[\"dropout_rate\"]\n","        self.attn_dropout_rate = config[\"attn_dropout_rate\"]\n","        self.num_layers = config[\"num_layers\"]\n","        self.hidden_dim = config[\"hidden_dim\"]\n","\n","         self.num_patches = int((self.img_dim // self.patch_dim) ** 3)\n","        self.seq_length = self.num_patches\n","        # self.flatten_dim = 128 * num_channels\n","        self.positional_encoding_type = config[\"positional_encoding_type\"] \n","        # self.linear_encoding = nn.Linear(self.flatten_dim, self.embedding_dim)\n","        if self.positional_encoding_type == \"learned\":\n","            self.position_encoding = LearnedPositionalEncoding(\n","                self.seq_length, self.embedding_dim, self.seq_length\n","            )\n","        elif self.positional_encoding_type == \"fixed\":\n","            self.position_encoding = FixedPositionalEncoding(\n","                self.embedding_dim,\n","            )\n","\n","        self.pe_dropout = nn.Dropout(p=self.dropout_rate)\n","\n","        self.transformer = TransformerModel(\n","            self.embedding_dim,\n","            self.num_layers,\n","            self.num_heads,\n","            self.hidden_dim,\n","\n","            self.dropout_rate,\n","            self.attn_dropout_rate,\n","        )\n","        self.pre_head_ln = nn.LayerNorm(self.embedding_dim)\n","\n","        # if self.conv_patch_representation:\n","\n","        self.conv_x = nn.Conv3d(\n","                256,\n","                self.embedding_dim,\n","                kernel_size=3,\n","                stride=1,\n","                padding=1\n","            )\n","\n","        # self.Unet = Unet(in_channels=4, base_channels=16, num_classes=4)\n","        # self.gn1 = nn.GroupNorm(num_groups=8, num_channels= 256)\n","        self.bn = nn.BatchNorm3d(256)\n","        self.relu = nn.ReLU(inplace=True)\n","        \n","        # Encoder Blocks\n","        self.in_conv0 = DownSampling(inChans=self.inChans, outChans=32, stride=1, dropout_rate=0.2)\n","        self.en_block0 = EncoderBlock(32, 32, activation=self.activation, normalizaiton=self.normalizaiton)\n","        self.en_down1 = DownSampling(32, 64)\n","        self.en_block1_0 = EncoderBlock(64, 64, activation=self.activation, normalizaiton=self.normalizaiton)\n","        self.en_block1_1 = EncoderBlock(64, 64, activation=self.activation, normalizaiton=self.normalizaiton)\n","        self.en_down2 = DownSampling(64, 128)\n","        self.en_block2_0 = EncoderBlock(128, 128, activation=self.activation, normalizaiton=self.normalizaiton)\n","        self.en_block2_1 = EncoderBlock(128, 128, activation=self.activation, normalizaiton=self.normalizaiton)\n","        self.en_down3 = DownSampling(128, 256)\n","        self.en_block3_0 = EncoderBlock(256, 256, activation=self.activation, normalizaiton=self.normalizaiton)\n","        self.en_block3_1 = EncoderBlock(256, 256, activation=self.activation, normalizaiton=self.normalizaiton)\n","        self.en_block3_2 = EncoderBlock(256, 256, activation=self.activation, normalizaiton=self.normalizaiton)\n","        self.en_block3_3 = EncoderBlock(256, 256, activation=self.activation, normalizaiton=self.normalizaiton)\n","\n","        # Decoder Blocks\n","        self.de_up2 = LinearUpSampling(256, 128, mode=self.mode)\n","        self.de_block2 = DecoderBlock(128, 128, activation=self.activation, normalizaiton=self.normalizaiton)\n","        self.de_up1 = LinearUpSampling(128, 64, mode=self.mode)\n","        self.de_block1 = DecoderBlock(64, 64, activation=self.activation, normalizaiton=self.normalizaiton)\n","        self.de_up0 = LinearUpSampling(64, 32, mode=self.mode)\n","        self.de_block0 = DecoderBlock(32, 32, activation=self.activation, normalizaiton=self.normalizaiton)\n","        self.de_end = OutputTransition(32, self.seg_outChans)\n","\n","        # Attention Blocks\n","        self.ag_2 = AttentionBlock(256, 128, 128)  # forward(g, x)\n","        self.ag_1 = AttentionBlock(128, 64, 64)\n","        self.ag_0 = AttentionBlock(64, 32, 32)\n","\n","        # Variational Auto-Encoder\n","        if self.config[\"VAE_enable\"]:\n","            self.dense_features = (self.input_shape[2]//16, self.input_shape[3]//16, self.input_shape[4]//16)\n","            self.vae = VAE(256, outChans=self.inChans - 3, dense_features=self.dense_features)   ### 7 > 4\n","\n","        self.Enblock8_1 = EnBlock1(in_channels=self.embedding_dim)\n","        self.Enblock8_2 = EnBlock2(in_channels=self.embedding_dim // 2)\n","    def decode(self,x_tr,intmd_x, intmd_layers=[1, 2, 3, 4]):\n","\n","        assert intmd_layers is not None, \"pass the intermediate layers for MLA\"\n","        encoder_outputs = {}\n","        all_keys = []\n","        for i in intmd_layers:\n","            val = str(2 * i - 1)\n","            _key = 'Z' + str(i)\n","            all_keys.append(_key)\n","            encoder_outputs[_key] = intmd_x[val]\n","        all_keys.reverse()\n","        return encoder_outputs[all_keys[0]]\n","\n","    def _reshape_output(self, x):\n","        x = x.view(\n","            x.size(0),\n","            int(self.input_shape[2] / self.patch_dim),\n","            int(self.input_shape[3] / self.patch_dim),\n","            int(self.input_shape[4] / self.patch_dim),\n","            self.embedding_dim,\n","        )\n","        x = x.permute(0, 4, 1, 2, 3).contiguous()\n","\n","        return x\n","\n","    def forward(self, x):\n","        out_init = self.in_conv0(x)  # (7, 128, 192, 160)\n","        out_en0 = self.en_block0(out_init)\n","        out_en1 = self.en_block1_1(self.en_block1_0(self.en_down1(out_en0))) \n","        out_en2 = self.en_block2_1(self.en_block2_0(self.en_down2(out_en1)))\n","        out_en3 = self.en_block3_3(self.en_block3_2(self.en_block3_1(self.en_block3_0(self.en_down3(out_en2)))))\n","\n","        transformer_input = out_en3\n","        print(f\"Transformer_input: {transformer_input.shape}\")\n","        x_tr = self.bn(transformer_input) #x = transformer_input\n","        x_tr = self.relu(x_tr)\n","        x_tr = self.conv_x(x_tr)\n","        x_tr = x_tr.permute(0, 2, 3, 4, 1).contiguous()\n","        x_tr = x_tr.view(x_tr.size(0), -1, self.embedding_dim)\n","        print(f\"Before position embedding: {x_tr.shape}\")\n","        x_tr = self.position_encoding(x_tr)\n","        x_tr = self.pe_dropout(x_tr)\n","\n","        # apply transformer\n","        x_tr, intmd_x = self.transformer(x_tr)\n","        x_tr = self.pre_head_ln(x_tr)\n","\n"," \n","\n","        x8 = self.decode(x_tr,intmd_x,intmd_layers=[1, 2, 3, 4])\n","\n","        x8 = self._reshape_output(x8)\n","        print(f\"End_Trans_After_rehape: {x8.shape}\")\n","        x8 = self.Enblock8_1(x8)\n","        x8 = self.Enblock8_2(x8)\n","        print(f\"After Enblocks_Transformer Output: {x8.shape}\")\n","        \n","        out_de2 = self.de_block2(self.de_up2(out_en3, self.ag_2(x8, out_en2)))\n","        out_de1 = self.de_block1(self.de_up1(out_de2, self.ag_1(out_de2, out_en1)))\n","        out_de0 = self.de_block0(self.de_up0(out_de1, self.ag_0(out_de1, out_en0)))\n","\n","        out_end = self.de_end(out_de0)\n","        \n","        if self.config[\"VAE_enable\"]:\n","            out_vae, out_distr = self.vae(x8)\n","            out_final = torch.cat((out_end, out_vae), 1)\n","            return out_final, out_distr\n","        \n","        return out_end\n","\n","class AttentionVNetForVisual(AttentionVNet):\n","    def __init__(self, config):\n","        super(AttentionVNetForVisual, self).__init__(config)\n","\n","    def forward(self, x):\n","        out_init = self.in_conv0(x)  # (7, 128, 192, 160)\n","        out_en0 = self.en_block0(out_init)  # 32 * 128^3\n","        out_en1 = self.en_block1_1(self.en_block1_0(self.en_down1(out_en0)))  # 64 * 64^3\n","        out_en2 = self.en_block2_1(self.en_block2_0(self.en_down2(out_en1)))  # 128 * 32^3\n","        out_en3 = self.en_block3_3(\n","            self.en_block3_2(\n","                self.en_block3_1(\n","                    self.en_block3_0(\n","                        self.en_down3(out_en2)))))  # 256 * 16^3\n","\n","        out_de2 = self.de_block2(self.de_up2(out_en3, self.ag_2(out_en3, out_en2)))  # 128 * 32^3\n","        out_de1 = self.de_block1(self.de_up1(out_de2, self.ag_1(out_de2, out_en1)))  # 64 * 64^3\n","        att_matrix = self.ag_0(out_de1, out_en0, True)  # 32 * 128^3\n","        return att_matrix"],"metadata":{"id":"8lliRdh59zPn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"J59xBHtWCZnK","executionInfo":{"status":"ok","timestamp":1651231471923,"user_tz":-360,"elapsed":152987,"user":{"displayName":"Prasun Datta","userId":"15897353971936605969"}},"outputId":"28ac7be9-2c49-4acd-ccf4-290fc3123cd7"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting batchgenerators==0.20.1\n","  Downloading batchgenerators-0.20.1.tar.gz (46 kB)\n","\u001b[K     || 46 kB 2.7 MB/s \n","\u001b[?25hCollecting certifi==2016.2.28\n","  Downloading certifi-2016.2.28-py2.py3-none-any.whl (366 kB)\n","\u001b[K     || 366 kB 14.2 MB/s \n","\u001b[?25hCollecting cycler==0.10.0\n","  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n","Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 4)) (4.4.2)\n","Collecting et-xmlfile==1.0.1\n","  Downloading et_xmlfile-1.0.1.tar.gz (8.4 kB)\n","Collecting future==0.18.2\n","  Downloading future-0.18.2.tar.gz (829 kB)\n","\u001b[K     || 829 kB 55.4 MB/s \n","\u001b[?25hCollecting imageio==2.9.0\n","  Downloading imageio-2.9.0-py3-none-any.whl (3.3 MB)\n","\u001b[K     || 3.3 MB 36.6 MB/s \n","\u001b[?25hCollecting jdcal==1.4.1\n","  Downloading jdcal-1.4.1-py2.py3-none-any.whl (9.5 kB)\n","Collecting joblib==0.15.1\n","  Downloading joblib-0.15.1-py3-none-any.whl (298 kB)\n","\u001b[K     || 298 kB 63.0 MB/s \n","\u001b[?25hCollecting kiwisolver==1.2.0\n","  Downloading kiwisolver-1.2.0-cp37-cp37m-manylinux1_x86_64.whl (88 kB)\n","\u001b[K     || 88 kB 7.6 MB/s \n","\u001b[?25hCollecting linecache2==1.0.0\n","  Downloading linecache2-1.0.0-py2.py3-none-any.whl (12 kB)\n","Collecting matplotlib==3.2.1\n","  Downloading matplotlib-3.2.1-cp37-cp37m-manylinux1_x86_64.whl (12.4 MB)\n","\u001b[K     || 12.4 MB 31.7 MB/s \n","\u001b[?25hCollecting networkx==2.5\n","  Downloading networkx-2.5-py3-none-any.whl (1.6 MB)\n","\u001b[K     || 1.6 MB 49.2 MB/s \n","\u001b[?25hCollecting nibabel==3.1.0\n","  Downloading nibabel-3.1.0-py3-none-any.whl (3.3 MB)\n","\u001b[K     || 3.3 MB 48.2 MB/s \n","\u001b[?25hCollecting nilearn==0.6.2\n","  Downloading nilearn-0.6.2-py3-none-any.whl (2.5 MB)\n","\u001b[K     || 2.5 MB 39.9 MB/s \n","\u001b[?25hCollecting numpy==1.18.4\n","  Downloading numpy-1.18.4-cp37-cp37m-manylinux1_x86_64.whl (20.2 MB)\n","\u001b[K     || 20.2 MB 49.7 MB/s \n","\u001b[?25hCollecting opencv-python==4.2.0.34\n","  Downloading opencv_python-4.2.0.34-cp37-cp37m-manylinux1_x86_64.whl (28.2 MB)\n","\u001b[K     || 28.2 MB 47.3 MB/s \n","\u001b[?25hCollecting openpyxl==3.0.4\n","  Downloading openpyxl-3.0.4-py2.py3-none-any.whl (241 kB)\n","\u001b[K     || 241 kB 61.8 MB/s \n","\u001b[?25hCollecting packaging==20.4\n","  Downloading packaging-20.4-py2.py3-none-any.whl (37 kB)\n","Collecting pandas==1.0.5\n","  Downloading pandas-1.0.5-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n","\u001b[K     || 10.1 MB 35.5 MB/s \n","\u001b[?25hCollecting Pillow==7.0.0\n","  Downloading Pillow-7.0.0-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n","\u001b[K     || 2.1 MB 40.0 MB/s \n","\u001b[?25hCollecting protobuf==3.12.2\n","  Downloading protobuf-3.12.2-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n","\u001b[K     || 1.3 MB 45.9 MB/s \n","\u001b[?25hCollecting pyparsing==2.4.7\n","  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n","\u001b[K     || 67 kB 5.8 MB/s \n","\u001b[?25hCollecting python-dateutil==2.8.1\n","  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n","\u001b[K     || 227 kB 57.1 MB/s \n","\u001b[?25hCollecting pytz==2020.1\n","  Downloading pytz-2020.1-py2.py3-none-any.whl (510 kB)\n","\u001b[K     || 510 kB 58.1 MB/s \n","\u001b[?25hCollecting PyWavelets==1.1.1\n","  Downloading PyWavelets-1.1.1-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)\n","\u001b[K     || 4.4 MB 26.1 MB/s \n","\u001b[?25hCollecting scikit-image==0.17.2\n","  Downloading scikit_image-0.17.2-cp37-cp37m-manylinux1_x86_64.whl (12.5 MB)\n","\u001b[K     || 12.5 MB 53.1 MB/s \n","\u001b[?25hCollecting scikit-learn==0.23.1\n","  Downloading scikit_learn-0.23.1-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n","\u001b[K     || 6.8 MB 34.7 MB/s \n","\u001b[?25hRequirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 29)) (1.4.1)\n","Collecting seaborn==0.11.0\n","  Downloading seaborn-0.11.0-py3-none-any.whl (283 kB)\n","\u001b[K     || 283 kB 63.7 MB/s \n","\u001b[?25hCollecting SimpleITK==1.1.0\n","  Downloading SimpleITK-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (41.0 MB)\n","\u001b[K     || 41.0 MB 120 kB/s \n","\u001b[?25hCollecting siren-pytorch==0.0.3\n","  Downloading siren_pytorch-0.0.3-py3-none-any.whl (3.2 kB)\n","Requirement already satisfied: six==1.15.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 33)) (1.15.0)\n","Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 34)) (0.0)\n","Collecting tensorboardX==1.4\n","  Downloading tensorboardX-1.4-py2.py3-none-any.whl (67 kB)\n","\u001b[K     || 67 kB 5.8 MB/s \n","\u001b[?25hCollecting threadpoolctl==2.0.0\n","  Downloading threadpoolctl-2.0.0-py3-none-any.whl (34 kB)\n","Collecting tifffile==2020.9.3\n","  Downloading tifffile-2020.9.3-py3-none-any.whl (148 kB)\n","\u001b[K     || 148 kB 52.6 MB/s \n","\u001b[?25hCollecting torch==1.6.0\n","  Downloading torch-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (748.8 MB)\n","\u001b[K     || 748.8 MB 17 kB/s \n","\u001b[?25hCollecting torchvision==0.7.0\n","  Downloading torchvision-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.9 MB)\n","\u001b[K     || 5.9 MB 55.8 MB/s \n","\u001b[?25hCollecting tqdm==4.26.0\n","  Downloading tqdm-4.26.0-py2.py3-none-any.whl (43 kB)\n","\u001b[K     || 43 kB 2.0 MB/s \n","\u001b[?25hCollecting traceback2==1.4.0\n","  Downloading traceback2-1.4.0-py2.py3-none-any.whl (16 kB)\n","Collecting unittest2==1.1.0\n","  Downloading unittest2-1.1.0-py2.py3-none-any.whl (96 kB)\n","\u001b[K     || 96 kB 1.8 MB/s \n","\u001b[?25hCollecting xlrd==1.2.0\n","  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n","\u001b[K     || 103 kB 63.2 MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf==3.12.2->-r /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/requirement.txt (line 22)) (57.4.0)\n","Collecting argparse\n","  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n","Building wheels for collected packages: batchgenerators, et-xmlfile, future\n","  Building wheel for batchgenerators (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for batchgenerators: filename=batchgenerators-0.20.1-py3-none-any.whl size=70312 sha256=90ba9e92c535417709301e294e75f2385d175da7aa745061e578177e03f1e067\n","  Stored in directory: /root/.cache/pip/wheels/8f/2d/85/59d9152c6b096fa8357bf56c7012591a4a31ceafff2b600ba7\n","  Building wheel for et-xmlfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for et-xmlfile: filename=et_xmlfile-1.0.1-py3-none-any.whl size=8916 sha256=a67316a58d8fb422e3692af3b58810644fb3f0156a6f3dc872908cae84e6b41e\n","  Stored in directory: /root/.cache/pip/wheels/e2/bd/55/048b4fd505716c4c298f42ee02dffd9496bb6d212b266c7f31\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=cf90c6e8e82a969753546f6b5b341524df7968eb216eaf7ef6073c7aa02bb3d8\n","  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n","Successfully built batchgenerators et-xmlfile future\n","Installing collected packages: numpy, threadpoolctl, python-dateutil, pyparsing, Pillow, linecache2, kiwisolver, joblib, cycler, traceback2, tifffile, scikit-learn, PyWavelets, pytz, packaging, networkx, matplotlib, imageio, future, argparse, unittest2, torch, scikit-image, protobuf, pandas, nibabel, jdcal, et-xmlfile, xlrd, tqdm, torchvision, tensorboardX, siren-pytorch, SimpleITK, seaborn, openpyxl, opencv-python, nilearn, certifi, batchgenerators\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.21.6\n","    Uninstalling numpy-1.21.6:\n","      Successfully uninstalled numpy-1.21.6\n","  Attempting uninstall: threadpoolctl\n","    Found existing installation: threadpoolctl 3.1.0\n","    Uninstalling threadpoolctl-3.1.0:\n","      Successfully uninstalled threadpoolctl-3.1.0\n","  Attempting uninstall: python-dateutil\n","    Found existing installation: python-dateutil 2.8.2\n","    Uninstalling python-dateutil-2.8.2:\n","      Successfully uninstalled python-dateutil-2.8.2\n","  Attempting uninstall: pyparsing\n","    Found existing installation: pyparsing 3.0.8\n","    Uninstalling pyparsing-3.0.8:\n","      Successfully uninstalled pyparsing-3.0.8\n","  Attempting uninstall: Pillow\n","    Found existing installation: Pillow 7.1.2\n","    Uninstalling Pillow-7.1.2:\n","      Successfully uninstalled Pillow-7.1.2\n","  Attempting uninstall: kiwisolver\n","    Found existing installation: kiwisolver 1.4.2\n","    Uninstalling kiwisolver-1.4.2:\n","      Successfully uninstalled kiwisolver-1.4.2\n","  Attempting uninstall: joblib\n","    Found existing installation: joblib 1.1.0\n","    Uninstalling joblib-1.1.0:\n","      Successfully uninstalled joblib-1.1.0\n","  Attempting uninstall: cycler\n","    Found existing installation: cycler 0.11.0\n","    Uninstalling cycler-0.11.0:\n","      Successfully uninstalled cycler-0.11.0\n","  Attempting uninstall: tifffile\n","    Found existing installation: tifffile 2021.11.2\n","    Uninstalling tifffile-2021.11.2:\n","      Successfully uninstalled tifffile-2021.11.2\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.0.2\n","    Uninstalling scikit-learn-1.0.2:\n","      Successfully uninstalled scikit-learn-1.0.2\n","  Attempting uninstall: PyWavelets\n","    Found existing installation: PyWavelets 1.3.0\n","    Uninstalling PyWavelets-1.3.0:\n","      Successfully uninstalled PyWavelets-1.3.0\n","  Attempting uninstall: pytz\n","    Found existing installation: pytz 2022.1\n","    Uninstalling pytz-2022.1:\n","      Successfully uninstalled pytz-2022.1\n","  Attempting uninstall: packaging\n","    Found existing installation: packaging 21.3\n","    Uninstalling packaging-21.3:\n","      Successfully uninstalled packaging-21.3\n","  Attempting uninstall: networkx\n","    Found existing installation: networkx 2.6.3\n","    Uninstalling networkx-2.6.3:\n","      Successfully uninstalled networkx-2.6.3\n","  Attempting uninstall: matplotlib\n","    Found existing installation: matplotlib 3.2.2\n","    Uninstalling matplotlib-3.2.2:\n","      Successfully uninstalled matplotlib-3.2.2\n","  Attempting uninstall: imageio\n","    Found existing installation: imageio 2.4.1\n","    Uninstalling imageio-2.4.1:\n","      Successfully uninstalled imageio-2.4.1\n","  Attempting uninstall: future\n","    Found existing installation: future 0.16.0\n","    Uninstalling future-0.16.0:\n","      Successfully uninstalled future-0.16.0\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.11.0+cu113\n","    Uninstalling torch-1.11.0+cu113:\n","      Successfully uninstalled torch-1.11.0+cu113\n","  Attempting uninstall: scikit-image\n","    Found existing installation: scikit-image 0.18.3\n","    Uninstalling scikit-image-0.18.3:\n","      Successfully uninstalled scikit-image-0.18.3\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.17.3\n","    Uninstalling protobuf-3.17.3:\n","      Successfully uninstalled protobuf-3.17.3\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 1.3.5\n","    Uninstalling pandas-1.3.5:\n","      Successfully uninstalled pandas-1.3.5\n","  Attempting uninstall: nibabel\n","    Found existing installation: nibabel 3.0.2\n","    Uninstalling nibabel-3.0.2:\n","      Successfully uninstalled nibabel-3.0.2\n","  Attempting uninstall: et-xmlfile\n","    Found existing installation: et-xmlfile 1.1.0\n","    Uninstalling et-xmlfile-1.1.0:\n","      Successfully uninstalled et-xmlfile-1.1.0\n","  Attempting uninstall: xlrd\n","    Found existing installation: xlrd 1.1.0\n","    Uninstalling xlrd-1.1.0:\n","      Successfully uninstalled xlrd-1.1.0\n","  Attempting uninstall: tqdm\n","    Found existing installation: tqdm 4.64.0\n","    Uninstalling tqdm-4.64.0:\n","      Successfully uninstalled tqdm-4.64.0\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.12.0+cu113\n","    Uninstalling torchvision-0.12.0+cu113:\n","      Successfully uninstalled torchvision-0.12.0+cu113\n","  Attempting uninstall: seaborn\n","    Found existing installation: seaborn 0.11.2\n","    Uninstalling seaborn-0.11.2:\n","      Successfully uninstalled seaborn-0.11.2\n","  Attempting uninstall: openpyxl\n","    Found existing installation: openpyxl 3.0.9\n","    Uninstalling openpyxl-3.0.9:\n","      Successfully uninstalled openpyxl-3.0.9\n","  Attempting uninstall: opencv-python\n","    Found existing installation: opencv-python 4.1.2.30\n","    Uninstalling opencv-python-4.1.2.30:\n","      Successfully uninstalled opencv-python-4.1.2.30\n","  Attempting uninstall: certifi\n","    Found existing installation: certifi 2021.10.8\n","    Uninstalling certifi-2021.10.8:\n","      Successfully uninstalled certifi-2021.10.8\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n","yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.23.1 which is incompatible.\n","torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.6.0 which is incompatible.\n","torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.6.0 which is incompatible.\n","tensorflow 2.8.0 requires numpy>=1.20, but you have numpy 1.18.4 which is incompatible.\n","tensorflow-metadata 1.7.0 requires protobuf<4,>=3.13, but you have protobuf 3.12.2 which is incompatible.\n","tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.4 which is incompatible.\n","spacy 2.2.4 requires tqdm<5.0.0,>=4.38.0, but you have tqdm 4.26.0 which is incompatible.\n","requests 2.23.0 requires certifi>=2017.4.17, but you have certifi 2016.2.28 which is incompatible.\n","panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.26.0 which is incompatible.\n","kapre 0.3.7 requires numpy>=1.18.5, but you have numpy 1.18.4 which is incompatible.\n","jaxlib 0.3.2+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.18.4 which is incompatible.\n","jax 0.3.4 requires numpy>=1.19, but you have numpy 1.18.4 which is incompatible.\n","imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.23.1 which is incompatible.\n","google-colab 1.0.0 requires pandas>=1.1.0; python_version >= \"3.0\", but you have pandas 1.0.5 which is incompatible.\n","fbprophet 0.7.1 requires tqdm>=4.36.1, but you have tqdm 4.26.0 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n","bokeh 2.3.3 requires pillow>=7.1.0, but you have pillow 7.0.0 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed Pillow-7.0.0 PyWavelets-1.1.1 SimpleITK-1.1.0 argparse-1.4.0 batchgenerators-0.20.1 certifi-2016.2.28 cycler-0.10.0 et-xmlfile-1.0.1 future-0.18.2 imageio-2.9.0 jdcal-1.4.1 joblib-0.15.1 kiwisolver-1.2.0 linecache2-1.0.0 matplotlib-3.2.1 networkx-2.5 nibabel-3.1.0 nilearn-0.6.2 numpy-1.18.4 opencv-python-4.2.0.34 openpyxl-3.0.4 packaging-20.4 pandas-1.0.5 protobuf-3.12.2 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2020.1 scikit-image-0.17.2 scikit-learn-0.23.1 seaborn-0.11.0 siren-pytorch-0.0.3 tensorboardX-1.4 threadpoolctl-2.0.0 tifffile-2020.9.3 torch-1.6.0 torchvision-0.7.0 tqdm-4.26.0 traceback2-1.4.0 unittest2-1.1.0 xlrd-1.2.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL","argparse","cycler","dateutil","google","kiwisolver","matplotlib","mpl_toolkits","numpy","pyparsing"]}}},"metadata":{}}]},{"cell_type":"code","source":["%cd /content/gdrive/MyDrive/2Stage_VAE/two-stage-VAE-Attention-gate-BraTS2020/Stage2_AttVAE\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sd17lPXUCbWV","executionInfo":{"status":"ok","timestamp":1651231508089,"user_tz":-360,"elapsed":12,"user":{"displayName":"Prasun Datta","userId":"15897353971936605969"}},"outputId":"30cf43a5-843f-4a4b-f1b1-c137ab00b3bb"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/.shortcut-targets-by-id/1ulOoNYuXyBUqnpz3wi6JGwAVg6ZkLT45/two-stage-VAE-Attention-gate-BraTS2020/Stage2_AttVAE\n","attVnet_add.py\t  dataset.py\t main_multi.py\tpredict_tta.py\ttrain.py\n","config.py\t  ensemble.py\t main.py\t__pycache__\tutils.py\n","dataset_multi.py  evaluation.py  metrics.py\truns\t\tvalidation.py\n"]}]},{"cell_type":"code","source":["!python attVnet_add.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CD-1ciqjD1t_","executionInfo":{"status":"ok","timestamp":1651231720689,"user_tz":-360,"elapsed":5406,"user":{"displayName":"Prasun Datta","userId":"15897353971936605969"}},"outputId":"ddc974c0-5d7b-4a57-f985-4c132d624580"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Transformer_input: torch.Size([1, 256, 16, 24, 20])\n","Before position embedding: torch.Size([1, 7680, 512])\n","End_Trans_After_rehape: torch.Size([1, 512, 16, 24, 20])\n","After Enblocks_Transformer Output: torch.Size([1, 256, 16, 24, 20])\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3121: UserWarning: Default upsampling behavior when mode=trilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode))\n","torch.Size([1, 4, 128, 192, 160])\n"]}]}]}